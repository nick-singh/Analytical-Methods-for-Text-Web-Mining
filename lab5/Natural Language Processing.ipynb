{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing\n",
    "\n",
    "NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n",
    "\n",
    "\n",
    "## Text Preprocessing\n",
    "\n",
    "Since, text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n",
    "\n",
    "It is predominantly comprised of three steps:\n",
    "\n",
    "* Noise Removal\n",
    "* Lexicon Normalization\n",
    "* Object Standardization\n",
    "\n",
    "### Noise Removal\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
    "\n",
    "For example – language stopwords (commonly used words of a language – is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n",
    "\n",
    "A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample code to remove noisy words from a text\n",
    "\n",
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"this is a sample text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use the regular expressions while dealing with special patterns of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from the '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sample code to remove a regex pattern \n",
    "import re \n",
    "\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"  \n",
    "\n",
    "_remove_regex(\"remove this #hashtag from the #tweet\", regex_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Normalization\n",
    "\n",
    "Another type of textual noise is about the multiple representations exhibited by single word.\n",
    "\n",
    "For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any NLP model.\n",
    "\n",
    "The most common lexicon normalization practices are :\n",
    "\n",
    "* **Stemming**:  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "* **Lemmatization**: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'multiply'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\" \n",
    "lem.lemmatize(word, \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'multipli'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Standardization\n",
    "\n",
    "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
    "\n",
    "Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted tweet by Nicholas'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "    new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "\n",
    "_lookup_words(\"RT this is a retweeted tweet by Nicholas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of available text data is highly unstructured and noisy in nature – to achieve better insights or to build better algorithms, it is necessary to play with clean data. For example, social media data is highly unstructured – it is an informal communication – typos, bad grammar, usage of slang, presence of unwanted content like URLs, Stopwords, Expressions etc. are the usual suspects.\n",
    "\n",
    "As a typical business problem, assume you are interested in finding:  which are the features of an iPhone which are more popular among the fans. You have extracted consumer opinions related to iPhone and here is a tweet you extracted:\n",
    "\n",
    "```I luv my &lt;3 iphone &amp; you're awsm apple. DisplayIsAwesome, sooo happppppy :) http://www.apple.com```\n",
    "\n",
    "\n",
    "Steps for data cleaning:\n",
    "\n",
    "**Escaping HTML characters**: Data obtained from web usually contains a lot of html entities like &lt; &gt; &amp; which gets embedded in the original data. It is thus necessary to get rid of these entities. One approach is to directly remove them by the use of specific regular expressions. Another approach is to use appropriate packages and modules (for example htmlparser of Python), which can convert these entities to standard html tags. For example: &lt; is converted to “<” and &amp; is converted to “&”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"I luv my <3 iphone & you're awsm apple. DisplayIsAwesome, sooo happppppy :) http://www.apple.com\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "\n",
    "original_tweet = \"I luv my &lt;3 iphone &amp; you're awsm apple. DisplayIsAwesome, sooo happppppy :) http://www.apple.com\"\n",
    "\n",
    "tweet = html_parser.unescape(original_tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoding data**: This is the process of transforming information from complex symbols to simple and easier to understand characters. Text data may be subject to different forms of decoding like “Latin”, “UTF8” etc. Therefore, for better analysis, it is necessary to keep the complete data in standard encoding format. UTF-8 encoding is widely accepted and is recommended to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = original_tweet.decode(\"utf8\").encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I luv my &lt;3 iphone &amp; you're awsm apple. DisplayIsAwesome, sooo happppppy :) http://www.apple.com\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removal of Stop-words**: When data analysis needs to be data driven at the word level, the commonly occurring words (stop-words) should be removed. One can either create a long list of stop-words or one can use predefined language specific libraries.\n",
    "\n",
    "**Removal of Punctuations**: All the punctuation marks according to the priorities should be dealt with. For example: “.”, “,”,”?” are important punctuations that should be retained while others need to be removed.\n",
    "\n",
    "**Removal of Expressions**: Textual data (usually speech transcripts) may contain human expressions like [laughing], [Crying], [Audience paused]. These expressions are usually non relevant to content of the speech and hence need to be removed. Simple regular expression can be useful in this case.\n",
    "\n",
    "**Split Attached Words**: We humans in the social forums generate text data, which is completely informal in nature. Most of the tweets are accompanied with multiple attached words like RainyDay, PlayingInTheCold etc. These entities can be split into their normal forms using simple rules and regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I luv my &lt;3 iphone &amp; you're awsm apple.  Display Is Awesome, sooo happppppy :) http://www.apple.com\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = \" \".join(re.findall('[A-Z][^A-Z]*', original_tweet))\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing words**: Sometimes words are not in proper formats. For example: “I looooveee you” should be “I love you”. Simple rules and regular expressions can help solve these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I luv my &lt;3 iphone &amp; you're awsm apple. DisplayIsAwesome, soo happy :) http://ww.apple.com\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Features (Feature Engineering on text data)\n",
    "\n",
    "To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques – Syntactical Parsing, Entities / N-grams / word-based features, Statistical features, and word embeddings. Read on to understand these techniques in detail.\n",
    "\n",
    "\n",
    "**Dependency Trees** - Sentences are composed of some words sewed together. The relationship among the words in a sentence is determined by the basic dependency grammar. Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words). Every relation can be represented in the form of a triplet (relation, governor, dependent).\n",
    "\n",
    "**Part of speech tagging** - Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. Here is a list of all possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input text. (it provides several implementations, the default one is perceptron tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('in', 'IN'), ('today', 'NN'), (\"'s\", 'POS'), ('class', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"I am learning Natural Language Processing in today's class\"\n",
    "tokens = word_tokenize(text)\n",
    "print pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech tagging is used for many important purposes in NLP:\n",
    "\n",
    "**Word sense disambiguation**: Some language words have multiple meanings according to their usage. For example, in the two sentences below:\n",
    "\n",
    "1. \"Please book my flight for Delhi\"\n",
    "\n",
    "2. \"I am going to read this book in the flight\"\n",
    "\n",
    "\"Book\" is used with different context, however the part of speech tag for both of the cases are different. In sentence 1, the word “book” is used as verb, while in 2 it is used as noun. \n",
    "\n",
    "\n",
    "**Improving word-based features**: A learning model could learn different contexts of a word when used word as the features, however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:\n",
    "\n",
    "Sentence - \"book my flight, I will read this book\"\n",
    "\n",
    "Tokens - (\"book\", 2), (\"my\", 1), (\"flight\", 1), (\"I\", 1), (\"will\", 1), (\"read\", 1), (\"this\", 1)\n",
    "\n",
    "Tokens with POS – (\"book_VB\", 1), (\"my_PRP$\", 1), (\"flight_NN\", 1), (\"I_PRP\", 1), (\"will_MD\", 1), (\"read_VB\", 1), (\"this_DT\", 1), (\"book_NN\", 1)\n",
    "\n",
    "\n",
    "**Normalization and Lemmatization**: POS tags are the basis of lemmatization process for converting a word to its base form (lemma).\n",
    "\n",
    "\"**Efficient stopword removal**: POS tags are also useful in efficient removal of stopwords.\n",
    "\n",
    "For example, there are some tags which always define the low frequency / less important words of a language. For example: (IN - “within”, “upon”, “except”), (CD – “one”,”two”, “hundred”), (MD – “may”, “must” etc)\n",
    "\n",
    "\n",
    "\n",
    "### Entity Extraction (Entities as features)\n",
    "\n",
    "Entities are defined as the most important chunks of a sentence – noun phrases, verb phrases or both. Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing. The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights.\n",
    "\n",
    "Topic Modelling & Named Entity Recognition are the two key entity detection methods in NLP.\n",
    "\n",
    "\n",
    "#### Named Entity Recognition (NER)\n",
    "The process of detecting the named entities such as person names, location names, company names etc from the text is called as NER. For example :\n",
    "\n",
    "Sentence – Sergey Brin, the manager of Google Inc. is walking in the streets of New York.\n",
    "\n",
    "Named Entities –  ( “person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)\n",
    "\n",
    "A typical NER model consists of three blocks:\n",
    "\n",
    "**Noun phrase identification**: This step deals with extracting all the noun phrases from a text using dependency parsing and part of speech tagging.\n",
    "\n",
    "**Phrase classification**: This is the classification step in which all the extracted noun phrases are classified into respective categories (locations, names etc). Google Maps API provides a good path to disambiguate locations, Then, the open databases from dbpedia, wikipedia can be used to identify person names or company names. Apart from this, one can curate the lookup tables and dictionaries by combining information from different sources.\n",
    "\n",
    "**Entity disambiguation**: Sometimes it is possible that entities are misclassified, hence creating a validation layer on top of the results is useful. Use of knowledge graphs can be exploited for this purposes. The popular knowledge graphs are – Google Knowledge Graph, IBM Watson and Wikipedia. \n",
    "\n",
    "\n",
    "\n",
    "#### Topic Modeling\n",
    "\n",
    "Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "\n",
    "\n",
    "#### N-Grams as Features\n",
    "A combination of N words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words (Unigrams) as features. Also, bigrams (N = 2) are considered as the most important features of all the others. The following code generates bigram of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "generate_ngrams('this is a sample text', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Features\n",
    "\n",
    "Text data can also be quantified directly into numbers using several techniques described in this section:\n",
    "\n",
    "\n",
    "**Term Frequency - Inverse Document Frequency (TF - IDF)**\n",
    "TF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example – let say there is a dataset of N text documents, In any document “D”, TF and IDF will be defined as –\n",
    "\n",
    "**Term Frequency (TF)** - TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
    "\n",
    "**Inverse Document Frequency (IDF)** - IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.5844829010200651\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 1)\t0.34520501686496574\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (2, 4)\t0.444514311537431\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i.\n",
    "\n",
    "\n",
    "#### Count / Density / Readability Features\n",
    "Count or Density based features can also be used in models and analysis. These features might seem trivial but shows a great impact in learning models. Some of the features are: Word Count, Sentence Count, Punctuation Counts and Industry specific word counts. Other types of measures include readability measures such as syllable counts, smog index and flesch reading ease.\n",
    "\n",
    "\n",
    "#### Word Embedding (text vectors)\n",
    "\n",
    "Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks.\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) and [GloVe](http://nlp.stanford.edu/projects/glove/) are the two popular models to create word embedding of a text. These models takes a text corpus as input and produces the word vectors as output.\n",
    "\n",
    "Word2Vec model is composed of preprocessing module, a shallow neural network model called Continuous Bag of Words and another shallow neural network model called skip-gram. These models are widely used for all other nlp problems. It first constructs a vocabulary from the training corpus and then learns word embedding representations. Following code using gensim package prepares the word embedding as the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['john', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.23661329464331132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print model.similarity('data', 'science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.5001867e-03 -4.5279837e-03  3.2164674e-04 -4.4285697e-03\n",
      "  4.1545318e-03 -2.1649762e-03 -3.8046767e-03 -1.7539723e-03\n",
      "  5.9814251e-04  3.5325827e-03 -3.9156429e-03  5.8388693e-04\n",
      "  2.1662123e-03 -4.5341258e-03  1.5721378e-03  4.6008709e-03\n",
      " -2.1712466e-03 -1.1091464e-03 -3.1049079e-03 -3.8329777e-03\n",
      "  2.6114439e-03  6.9665466e-04  1.8230440e-03 -2.0439368e-04\n",
      " -1.3953244e-03  2.5326526e-03 -1.3467000e-03  7.9407363e-04\n",
      " -8.5678376e-04 -2.7677296e-03 -3.2886763e-03 -4.6619233e-03\n",
      "  7.3562146e-06  1.1412736e-03 -1.2158153e-03  7.6474954e-04\n",
      " -4.3854597e-03  1.7202114e-03  2.6836181e-03  1.4622611e-03\n",
      " -2.5849789e-03  4.8951223e-03  4.8026382e-03 -1.6887399e-03\n",
      "  3.7905199e-03 -1.8123538e-03 -1.8461549e-03 -3.6006782e-03\n",
      " -8.0820097e-04  2.4030278e-03  2.6205764e-03 -1.3611730e-03\n",
      "  3.2463272e-03  4.4065714e-03 -4.9645538e-03  1.0241962e-03\n",
      "  7.2724192e-04  1.0305879e-03  3.8043426e-03 -1.5256479e-03\n",
      "  4.4337765e-04  8.2238915e-04  1.0542222e-03  1.5848925e-04\n",
      "  2.7174465e-04 -4.1874815e-03 -1.3866873e-03 -4.4830837e-03\n",
      " -5.5923843e-04 -4.6276394e-03 -4.6137613e-03  1.1266705e-03\n",
      "  2.8460119e-03 -4.0153079e-03 -1.0213936e-03 -3.4116728e-03\n",
      " -4.3979678e-03 -2.1404165e-03  4.8894719e-03  1.2435411e-03\n",
      " -2.9593618e-03  6.9390453e-04 -1.7993891e-04  3.9033850e-03\n",
      "  3.8551667e-03 -3.4354749e-04 -1.7333762e-03 -3.4099706e-03\n",
      "  2.2529550e-03  7.4087334e-04  9.8270993e-04 -2.9371216e-04\n",
      " -2.5866383e-03  3.2008286e-03 -3.1004008e-03  4.4333171e-03\n",
      "  4.8331725e-03  1.0723709e-03 -5.6745659e-04 -2.0097558e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print model['learning'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important tasks of NLP\n",
    "\n",
    "#### Text Classification\n",
    "\n",
    "Text classification is one of the classical problem of NLP. Notorious examples include – Email Spam Identification, topic classification of news, sentiment classification and organization of web pages by search engines.\n",
    "\n",
    "Text classification, in common words is defined as a technique to systematically classify a text object (document or sentence) in one of the fixed category. It is really helpful when the amount of data is too large, especially for organizing, information filtering, and storage purposes.\n",
    "\n",
    "A typical natural language classifier consists of two parts: (a) Training (b) Prediction. Firstly the text input is processes and features are created. The machine learning models then learn these features and is used for predicting against the new text.\n",
    "\n",
    "#### Other NLP problems / tasks\n",
    "\n",
    "**Text Summarization** - Given a text article or paragraph, summarize it automatically to produce most important and relevant sentences in order.\n",
    "\n",
    "**Machine Translation** - Automatically translate text from one human language to another by taking care of grammar, semantics and information about the real world, etc.\n",
    "\n",
    "**Natural Language Generation and Understanding** - Convert information from computer databases or semantic intents into readable human language is called language generation. Converting chunks of text into more logical structures that are easier for computer programs to manipulate is called language understanding.\n",
    "\n",
    "**Optical Character Recognition** - Given an image representing printed text, determine the corresponding text.\n",
    "\n",
    "**Document to Information** - This involves parsing of textual data present in documents (websites, files, pdfs and images) to analyzable and clean format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
