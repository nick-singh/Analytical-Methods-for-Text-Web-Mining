{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping using Python\n",
    "\n",
    "In this tutorial, you'll learn how to extract data from the web, manipulate and clean data using Python's Pandas library.\n",
    "\n",
    "\n",
    "Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web. Whether you are a data scientist, engineer, or anybody who analyzes large amounts of datasets, the ability to scrape data from the web is a useful skill to have. Let's say you find data from the web, and there is no direct way to download it, web scraping using Python is a skill you can use to extract the data into a useful form that can be imported.\n",
    "\n",
    "#### Web Scraping using Beautiful Soup & UrlLib\n",
    "\n",
    "To perform web scraping, you should also import the libraries shown below. The UrlLib module is used to open URLs and act as a browser. The Beautiful Soup package is used to extract data from html files. The Beautiful Soup library's name is bs4 which stands for Beautiful Soup, version 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime, re\n",
    "import urllib2 as urllib\n",
    "# import urllib.request # Python 3.X\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing necessary modules, you should specify the URL containing the dataset and set the necessary defaults. \n",
    "\n",
    "* The Headers are some standard header parameters that a browser expects\n",
    "* The base keys are the query parameters that are used when searching the actual page \n",
    "```\n",
    "'d1':'2019-05-18',\n",
    "'s':'start_time',\n",
    "'sd':'desc',\n",
    "'l': 10,\n",
    "'t':'article',\n",
    "'nsa':'eedition' \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"https://www.trinidadexpress.com/search/?\"\n",
    "basekeys = ['d1', 's', 'sd', 'l', 't', 'nsa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is only for Python 3.X\n",
    "def get_url(url):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on how the site uses its search, we created a function ```build_daily_search``` to dynamically build a search query given the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_daily_search(keys = None, query = None):\n",
    "    q_list = []\n",
    "    for k in keys:\n",
    "        q_list.append(str(k)+\"=\"+str(query[k]))\n",
    "    q = \"&\".join(q_list)\n",
    "    return base+q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```process_link``` function carries out a couple of operations:\n",
    "\n",
    "```Python\n",
    "# e.g. /news/local/charged-with-killing-wife-s-ex-boyfriend/article_0dba43a0-7b39-11e9-a62e-ef0d1c3758f4.html\n",
    "#opens the above url and stores the html content in the variable link\n",
    "response = urllib.urlopen(\"https://www.trinidadexpress.com\"+url_link)\n",
    "link = response.read()\n",
    "#BeautifulSoup pares the html page to be processed\n",
    "soup = BeautifulSoup(link, 'html.parser') \n",
    "# to find the title, author, date created and image url we inspect the content of the page\n",
    "# and looks for the html attributes that are linked to the values we want \n",
    "# we then use beautifulSoup to find these values by their html attribues\n",
    "title = soup.find('h1', {'class', \"headline\"}).text.encode('ascii', 'ignore')\n",
    "author = soup.select('span[itemprop=\"author\"]')\n",
    "dateCreated = soup.find('time', {'class', \"asset-date text-muted\"})\n",
    "imgUrl = soup.find('div', {'class','image'})\n",
    "```\n",
    "\n",
    "Once we get the desired values we store them in a ```data``` variable. The next step is the get the actual text content from the site. This is done in a similar fasion as the other vaules.\n",
    "We investigate the ```html``` attributes that represent the values we want, in this case the attribute is the ```html <p>``` paragraph tag.\n",
    "\n",
    "We get all paragraph tags and extract the text from it. Once we obtain all the text we add it to the ```data``` variable and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_link(url_link):\n",
    "    print(\"processing: \", url_link)\n",
    "    try:\n",
    "#         link = get_url(\"https://www.trinidadexpress.com\"+url_link) # Python3.X\n",
    "        response = urllib.urlopen(\"https://www.trinidadexpress.com\"+url_link)\n",
    "        link = response.read()\n",
    "        soup = BeautifulSoup(link, 'html.parser')\n",
    "        title = soup.find('h1', {'class', \"headline\"}).text.encode('ascii', 'ignore')\n",
    "        author = soup.select('span[itemprop=\"author\"]')\n",
    "        dateCreated = soup.find('time', {'class', \"asset-date text-muted\"})\n",
    "        imgUrl = soup.find('div', {'class','image'})\n",
    "        data = {\n",
    "            'url':url_link,\n",
    "            'title': re.sub('\\W+ ','', title),\n",
    "            'author':author[0].text.encode('ascii', 'ignore') if len(author) > 0 else \"undefined\",\n",
    "            'dateCreated': dateCreated['datetime'] if dateCreated is not None\\\n",
    "                            else datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'imgUrl': imgUrl.find('img')['src'] if imgUrl is not None else \"undefined\"\n",
    "        }\n",
    "        article = soup.find('div', {'class':'asset-content subscriber-premium'}).find_all('p')\n",
    "        article_text = \"\"\n",
    "        for pgraph in article:\n",
    "            article_text+=pgraph.text.encode('ascii', 'ignore').replace('\\n','')\n",
    "        if imgUrl is not None:\n",
    "            caption = soup.find('figcaption', {'class', \"caption\"})\n",
    "            data['caption'] = re.sub('\\W+ ','', caption.text.encode('ascii', 'ignore')) \\\n",
    "            if caption is not None else \"undefined\"\n",
    "        data['article_text'] = article_text\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a function above which when given a link, opens the link, views the content and extracts the desired content, however if we want to dynamically get all links that the main page has we would need to build a seperate function for that.\n",
    "\n",
    "The ```get_page_links``` function takes care of that for us. Given the query parameters outlined above we build the url in the format that the webpage expects it then same as above we:\n",
    "\n",
    "* Open the link \n",
    "```\n",
    "response = urllib.urlopen(url)\n",
    "open_url = response.read()\n",
    "```\n",
    "* Parse the result for processing ```soup = BeautifulSoup(open_url.read(), 'html.parser')```\n",
    "* Get the attribute that has the links we want ```content = soup.find(class_=\"results-container\")```\n",
    "* And for all the attributes in the page extract the links:\n",
    "```Python\n",
    "articles = []\n",
    "for link in link_containers:\n",
    "l = link.find('a')['href']\n",
    "article = process_link(l)\n",
    "articles.append(article)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_links(query):\n",
    "    print(query)\n",
    "    q = query\n",
    "    k = query.keys()\n",
    "    url = build_daily_search(k, q)\n",
    "    print(url)\n",
    "    response = urllib.urlopen(url)\n",
    "    open_url = response.read()\n",
    "    #open_url = get_url(url) #Python 3.X\n",
    "    soup = BeautifulSoup(open_url, 'html.parser')\n",
    "    links = []\n",
    "    content = soup.find(class_=\"results-container\")\n",
    "    link_containers = content.find_all('h3', {'class', \"tnt-headline\"})\n",
    "    articles = []\n",
    "    for link in link_containers:\n",
    "        l = link.find('a')['href']\n",
    "        article = process_link(l)\n",
    "        articles.append(article)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = {\n",
    "        'd1':'2019-05-20', #start date\n",
    "        's':'start_time', #parameter name\n",
    "        'sd':'desc', # order\n",
    "        'l': 10, # number of links to display\n",
    "        't':'article', # type of articles\n",
    "        'nsa':'eedition' # parameter name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nsa': 'eedition', 'l': 10, 's': 'start_time', 't': 'article', 'd1': '2019-05-20', 'sd': 'desc'}\n",
      "https://www.trinidadexpress.com/search/?nsa=eedition&l=10&s=start_time&t=article&d1=2019-05-20&sd=desc\n",
      "('processing: ', u'/business/local/sando-waterfront-project-begins/article_00e69a40-7c22-11e9-9e99-574806223954.html')\n",
      "('processing: ', u'/sports/local/getting-started/article_31bb254a-7c1e-11e9-b2ce-db2c9b90612a.html')\n",
      "('processing: ', u'/sports/local/club-sando-la-horquetta-win-ypl-titles/article_98140696-7c1d-11e9-a0b4-4fc3ac0bf4cd.html')\n",
      "('processing: ', u'/sports/local/combat-gold-for-t-t/article_eba59f8c-7c1c-11e9-b647-e7d9ec57b764.html')\n",
      "('processing: ', u'/sports/local/costello-ross-cop-caribbean-age-group-triathlon-titles/article_46461748-7c1b-11e9-9895-db55dee68deb.html')\n",
      "('processing: ', u'/features/local/shastri-s-arrival/article_eebcc0e0-7c19-11e9-9ec3-c3e3a4f2161c.html')\n",
      "('processing: ', u'/opinion/columnists/regional-foreign-policy-imperatives/article_25c8308e-7c19-11e9-b4bf-2bccd5b73c4c.html')\n",
      "('processing: ', u'/opinion/editorials/take-firm-action-against-abusive-cops/article_63093466-7c19-11e9-bf96-d308d694feac.html')\n",
      "('processing: ', u'/opinion/letters/modernise-the-workers-too/article_7779874e-7c18-11e9-8b51-5738dd120418.html')\n",
      "('processing: ', u'/opinion/columnists/caricom-must-unify-to-resist-imposition-of-oas-boss/article_a366774a-7c18-11e9-a25f-f32d228d6c47.html')\n"
     ]
    }
   ],
   "source": [
    "end_data = get_page_links(qry)\n",
    "df = pd.DataFrame(end_data)\n",
    "df.to_csv(qry['d1']+'-'+str(qry['l'])+'.csv', sep='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
